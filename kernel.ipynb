{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom tqdm import tqdm\nimport os\n\n#Functions to save objects for later use and retireve it\nimport pickle\ndef savetofile(obj,filename):\n    pickle.dump(obj,open(filename+\".p\",\"wb\"))\ndef openfromfile(filename):\n    temp = pickle.load(open(filename+\".p\",\"rb\"))\n    return temp",
      "execution_count": 84,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv(\"../input/train.csv\", names = [\"Label\", \"Headline\", \"Description\"])\ndf_test = pd.read_csv(\"../input/test.csv\", names = [\"Label\", \"Headline\", \"Description\"])",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02ad34b5c75c1bfefcac62e20c378719280befc8"
      },
      "cell_type": "code",
      "source": "df_train_label =  df_train[\"Label\"]\ndf_test_label =  df_test[\"Label\"]",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df45f1097f82e9808ce532c84b1d323409ee7af0"
      },
      "cell_type": "code",
      "source": "print(df_train[\"Label\"].value_counts())\nprint(df_test[\"Label\"].value_counts())",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": "4    30000\n3    30000\n2    30000\n1    30000\nName: Label, dtype: int64\n3    1900\n2    1900\n1    1900\n4    1900\nName: Label, dtype: int64\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c919343aac3a54ff32f6e33dc7d8fca57bf788fa"
      },
      "cell_type": "markdown",
      "source": "## **Text Preprocessing**"
    },
    {
      "metadata": {
        "_uuid": "d99d4362a3034b28e3a7bc5d687206b9e5c9ecc9"
      },
      "cell_type": "markdown",
      "source": "1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d504d4baf61b60e792b00c9cde9192724fc55403"
      },
      "cell_type": "code",
      "source": "# printing some random Description\ndes_1 = df_train['Description'].values[51147]\nprint(des_1)\nprint(\"=\"*50)\n\ndes_2 = df_train['Description'].values[51151]\nprint(des_2)\nprint(\"=\"*50)\n\ndes_3 = df_train['Description'].values[51628]\nprint(des_3)\nprint(\"=\"*50)\n\ndes_4 = df_train['Description'].values[51867]\nprint(des_4)\nprint(\"=\"*50)",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reuters - A former Enron Corp.  \\assistant treasurer pleaded guilty on Tuesday in U.S. District\\Court to conspiracy to commit securities fraud for disguising\\the company's weak finances before its dramatic collapse.\n==================================================\n SAN FRANCISCO (Reuters) - A PeopleSoft Inc. &lt;A HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=PSFT.O target=/stocks/quickinfo/fullquote\"&gt;PSFT.O&lt;/A&gt; board  member told a Delaware court on Tuesday that the software maker  would consider a bid from its rival Oracle Corp. &lt;A HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=ORCL.O target=/stocks/quickinfo/fullquote\"&gt;ORCL.O&lt;/A&gt;  provided the price was right and a deal closed quickly.\n==================================================\n NEW YORK (Reuters) - U.S. stocks were set to open flat on  Wednesday as high crude oil prices and profit warnings weighed  on the market on the eve of a new earnings reporting season.\n==================================================\n WASHINGTON (Reuters) - The United States and Europe on  Wednesday filed tit-for-tat World Trade Organization complaints  over billions of dollars in subsidies for top aircraft  manufacturers Airbus and Boeing.\n==================================================\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "42cb57daf14d73cecb8cdca43d93ff5c85d693de"
      },
      "cell_type": "markdown",
      "source": "### **1. Removing url**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1263531c0b858d1b77e28bb459390e0d3c08be40"
      },
      "cell_type": "code",
      "source": "# remove urls from text python: https://stackoverflow.com/a/40823105/4084039\ndes_1 = re.sub(r\"http\\S+\", \"\", des_1)\ndes_2 = re.sub(r\"http\\S+\", \"\", des_2)\ndes_3 = re.sub(r\"http\\S+\", \"\", des_3)\ndes_4 = re.sub(r\"http\\S+\", \"\", des_4)\n\nprint(des_2)",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": " SAN FRANCISCO (Reuters) - A PeopleSoft Inc. &lt;A HREF=\" target=/stocks/quickinfo/fullquote\"&gt;PSFT.O&lt;/A&gt; board  member told a Delaware court on Tuesday that the software maker  would consider a bid from its rival Oracle Corp. &lt;A HREF=\" target=/stocks/quickinfo/fullquote\"&gt;ORCL.O&lt;/A&gt;  provided the price was right and a deal closed quickly.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b093da9f52cc1465882b8324a95dd4ec50c03569"
      },
      "cell_type": "markdown",
      "source": "### **2. Removing tags**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f453f8fb2ce49c237f8b5971f1d4da385f93a2d"
      },
      "cell_type": "code",
      "source": "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(des_1, 'lxml')\ntext = soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup = BeautifulSoup(des_2, 'lxml')\ntext = soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup = BeautifulSoup(des_3, 'lxml')\ntext = soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup = BeautifulSoup(des_4, 'lxml')\ntext = soup.get_text()\nprint(text)",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reuters - A former Enron Corp.  \\assistant treasurer pleaded guilty on Tuesday in U.S. District\\Court to conspiracy to commit securities fraud for disguising\\the company's weak finances before its dramatic collapse.\n==================================================\nSAN FRANCISCO (Reuters) - A PeopleSoft Inc. <A HREF=\" target=/stocks/quickinfo/fullquote\">PSFT.O</A> board  member told a Delaware court on Tuesday that the software maker  would consider a bid from its rival Oracle Corp. <A HREF=\" target=/stocks/quickinfo/fullquote\">ORCL.O</A>  provided the price was right and a deal closed quickly.\n==================================================\nNEW YORK (Reuters) - U.S. stocks were set to open flat on  Wednesday as high crude oil prices and profit warnings weighed  on the market on the eve of a new earnings reporting season.\n==================================================\nWASHINGTON (Reuters) - The United States and Europe on  Wednesday filed tit-for-tat World Trade Organization complaints  over billions of dollars in subsidies for top aircraft  manufacturers Airbus and Boeing.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "36cdc967081cac4a7f5cb780aa1cb10cc3f210c1"
      },
      "cell_type": "markdown",
      "source": "### **3. Exapanding english language**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea96d0bd6ff5fdb984f9910fa45568ca28a681a5"
      },
      "cell_type": "code",
      "source": "import re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f40ef23bd578a0122294404bef2c26d9861b901"
      },
      "cell_type": "markdown",
      "source": "### **4. Removing special charachters and numbers**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "240ba0da3da19ed6cb4c03d963d71482084d8ded"
      },
      "cell_type": "code",
      "source": "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\ndes_1 = re.sub(\"\\S*\\d\\S*\", \"\", des_1).strip()\nprint(des_1)",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reuters - A former Enron Corp.  \\assistant treasurer pleaded guilty on Tuesday in U.S. District\\Court to conspiracy to commit securities fraud for disguising\\the company's weak finances before its dramatic collapse.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a1bb7a3a82b4888a8447cf40371ef57376c36fe"
      },
      "cell_type": "code",
      "source": "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\ndes_1 = re.sub('[^A-Za-z0-9]+', ' ', des_1)\nprint(des_1)",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reuters A former Enron Corp assistant treasurer pleaded guilty on Tuesday in U S District Court to conspiracy to commit securities fraud for disguising the company s weak finances before its dramatic collapse \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "060e1935d23e7f0b5e16308e5fa0cf32feb8ad85"
      },
      "cell_type": "markdown",
      "source": "### **5. Removing Stopwords**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3495d57e1df54aeae55fb61b5592e1af57ef8835"
      },
      "cell_type": "code",
      "source": "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n# <br /><br /> ==> after the above steps, we are getting \"br br\"\n# we are including them into stop words list\n# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])",
      "execution_count": 69,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9eff19a808f784a32e417c0a89451667f1513079"
      },
      "cell_type": "code",
      "source": "# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_description = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(df_train['Description'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https://gist.github.com/sebleier/554280\n    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_description.append(sentence.strip())\n",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 120000/120000 [00:45<00:00, 2653.93it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "25313558daa0fbe9e689fad536f3c2810d3b7e0e"
      },
      "cell_type": "markdown",
      "source": "**Similarly we can do the preprocessiong for Headline**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5847119581e619edd7fd27b8471eba0936318af3"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\npreprocessed_headline = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(df_train['Headline'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https://gist.github.com/sebleier/554280\n    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_headline.append(sentence.strip())",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 120000/120000 [00:37<00:00, 3204.87it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "903519526e2e5b3b479092d171de3bdd670931cd"
      },
      "cell_type": "code",
      "source": "df_train_preprocessed = df_train\ndf_train_preprocessed['Label'] = df_train['Label']\ndf_train_preprocessed['Headline'] = preprocessed_headline\ndf_train_preprocessed['Description'] = preprocessed_description",
      "execution_count": 72,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea431f6f25357e5e4b0891905cfe2cd030315f45"
      },
      "cell_type": "code",
      "source": "df_train_preprocessed.head()",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 73,
          "data": {
            "text/plain": "   Label                        ...                                                                Description\n0      3                        ...                          reuters short sellers wall street dwindling ba...\n1      3                        ...                          reuters private investment firm carlyle group ...\n2      3                        ...                          reuters soaring crude prices plus worries econ...\n3      3                        ...                          reuters authorities halted oil export flows ma...\n4      3                        ...                          afp tearaway world oil prices toppling records...\n\n[5 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>Headline</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>wall st bears claw back black reuters</td>\n      <td>reuters short sellers wall street dwindling ba...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>carlyle looks toward commercial aerospace reuters</td>\n      <td>reuters private investment firm carlyle group ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>oil economy cloud stocks outlook reuters</td>\n      <td>reuters soaring crude prices plus worries econ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>iraq halts oil exports main southern pipeline ...</td>\n      <td>reuters authorities halted oil export flows ma...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>oil prices soar time record posing new menace ...</td>\n      <td>afp tearaway world oil prices toppling records...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "485658e92805b7b262ee16bd99b3de06130f4af2"
      },
      "cell_type": "markdown",
      "source": "## Test set Preprocessing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c47b0ad0887dc5605af79d4db50929653f4777d5"
      },
      "cell_type": "code",
      "source": "#preprocessing for test set headlines\nfrom tqdm import tqdm\npreprocessed_headline_test = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(df_test['Headline'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https://gist.github.com/sebleier/554280\n    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_headline_test.append(sentence.strip())\n\n#preprocessing for test set descriptions    \nfrom tqdm import tqdm\npreprocessed_description_test = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(df_test['Description'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https://gist.github.com/sebleier/554280\n    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_description_test.append(sentence.strip())",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 7600/7600 [00:02<00:00, 3192.03it/s]\n100%|██████████| 7600/7600 [00:02<00:00, 2701.25it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89155819ab6b2cfc959d71a891899680bfa8a3ea"
      },
      "cell_type": "code",
      "source": "df_test_preprocessed = df_test\ndf_test_preprocessed['Label'] = df_test['Label']\ndf_test_preprocessed['Headline'] = preprocessed_headline_test\ndf_test_preprocessed['Description'] = preprocessed_description_test",
      "execution_count": 75,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e303fa1a1db751fb32c660246c8713d43c04df0"
      },
      "cell_type": "markdown",
      "source": "# **Logistic Regression Model**"
    },
    {
      "metadata": {
        "_uuid": "41f5e402e6cb06e28f7898f23b60ed44e1c1d22e"
      },
      "cell_type": "markdown",
      "source": "## Bi-Grams and n-Grams\nSimply,Converting a collection of text documents to a matrix of token counts"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19752c2ca301a1bc1e7cca5844a73584aafbe37a",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\n\n#removing stop words like \"not\" should be avoided before building n-grams\n#Vectorizing the description\ncount_vect_description = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\nfinal_bigram_des_train = count_vect_description.fit_transform(preprocessed_description)\n#Normalize\nfinal_bigram_des_train = preprocessing.normalize(final_bigram_des_train)\n\nfinal_bigram_des_test = count_vect_description.transform(preprocessed_description_test)\n#Normalize\nfinal_bigram_des_test = preprocessing.normalize(final_bigram_des_test)\n\n#Vectorizing the Headline\ncount_vect_headline = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\nfinal_bigram_head_train = count_vect_headline.fit_transform(preprocessed_headline)\n#Normalize\nfinal_bigram_head_train = preprocessing.normalize(final_bigram_head_train)\n\nfinal_bigram_head_test = count_vect_headline.transform(preprocessed_headline_test)\n#Normalize\nfinal_bigram_head_test = preprocessing.normalize(final_bigram_head_test)\n",
      "execution_count": 85,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d0cbb0bcaaae451ed79a214699ee56dbd6a116e5"
      },
      "cell_type": "markdown",
      "source": "Observed that Data Normalization gives better accuracy rather than Data Standardization. \nHence used Data Normalization"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd24ac8e5160503cd8ea7b2b44da7d445dcc6589"
      },
      "cell_type": "code",
      "source": "train_data = final_bigram_head_train + final_bigram_des_train\nprint(\"Train Data Size: \",train_data.shape)\ntest_data = final_bigram_head_test + final_bigram_des_test\nprint(\"Test Data Size: \", test_data.shape)",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train Data Size:  (120000, 5000)\nTest Data Size:  (7600, 5000)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e0a93de86900f9aa973cd74fb0e888d255912db9"
      },
      "cell_type": "markdown",
      "source": "### **Finding the best \"C\" or \"1/lambda\" and regularizer [ L1 or L2 ] using Randomized Search CV**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7837c84f58493cd6dad38e84e9423b9722a57855",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\n\nclf = LogisticRegression()\n#params we need to try on classifier\nparam_grid = { 'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n              'penalty':['l1','l2']}\nkf = KFold(n_splits=3) #For k fold splitting\ngsv = RandomizedSearchCV(clf,param_grid,cv= kf,verbose=1)\ngsv.fit(train_data,df_train_label)\nsavetofile(gsv,\"Log Reg/gsv_bi\")\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  5.7min finished\n",
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Log Reg/gsv_bi.p'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-6b2316b9a276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msavetofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Log Reg/gsv_bi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best HyperParameter: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Accuracy: %.2f%%\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-c11ed6a517fd>\u001b[0m in \u001b[0;36msavetofile\u001b[0;34m(obj, filename)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavetofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopenfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Log Reg/gsv_bi.p'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79174a26d9f6b178b72547dc31bbb1aebcc37778"
      },
      "cell_type": "code",
      "source": "#Testing Accuracy on Test data\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C= 0.5, penalty= 'l1')\nclf.fit(train_data,df_train_label)\ny_pred = clf.predict(test_data)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(df_test_label, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(df_test_label, y_pred, average = 'micro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(df_test_label, y_pred, average = 'micro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(df_test_label, y_pred,  average = 'micro')))\nprint(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\nprint(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\ndf_cm = pd.DataFrame(confusion_matrix(df_test_label, y_pred), range(4),range(4))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31ae1b5e49a336fc36fa8baf516c01e841299aa8"
      },
      "cell_type": "code",
      "source": "def plot_error_vs_c(gsv):\n    x1=[]\n    y1=[]\n    x2=[]\n    y2=[]\n    for a in gsv.cv_results_:\n        if (a[0]['penalty']) == 'l1':\n            y1.append(1-a[1])\n            x1.append(a[0]['C'])\n        else:\n            y2.append(1-a[1])\n            x2.append(a[0]['C'])\n    plt.xlim(-10,1010)\n    plt.ylim(0,0.2)\n    plt.xlabel(\"C\",fontsize=15)\n    plt.ylabel(\"Misclassification Error\")\n    plt.title('Misclassification Error v/s C')\n    plt.plot(x1,y1,'b',label=\"L1\")\n    plt.plot(x2,y2,'r',label=\"L2\")\n    plt.legend()\n    plt.show()\ngsv = openfromfile(\"Log Reg/gsv_bi\")\nplot_error_vs_c(gsv)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6491a5d2d1085f2567a2fa47459e35b99d15b1ff"
      },
      "cell_type": "markdown",
      "source": "## **TF-IDF**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8513b9457125afc18c57405b3446cca26952ee0"
      },
      "cell_type": "code",
      "source": "#Vectorizing the description\ntfidf_des = TfidfVectorizer(ngram_range=(1,2), min_df=10)\ntfidf_des_train= tfidf_des.fit_transform(preprocessed_description)\n#Normalize\ntfidf_des_train = preprocessing.normalize(tfidf_des_train)\n\ntfidf_des_test= tfidf_des.transform(preprocessed_description_test)\n#Normalize\ntfidf_des_test = preprocessing.normalize(tfidf_des_test)\n\n#Vectorizing the Headline\ntfidf_head = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\ntfidf_head_train = tfidf_head.fit_tranform(preprocessed_headline)\n#Normalize\ntfidf_head_train = preprocessing.normalize(tfidf_head_train)\n\ntfidf_head_test = tfidf_head.transform(preprocessed_headline_test)\n#Normalize\ntfidf_head_test = preprocessing.transform(tfidf_head_test)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bdb210f38fa7d7d4739d84cab19724a0260c611"
      },
      "cell_type": "code",
      "source": "train_data = tfidf_head_train + tfidf_des_train\nprint(train_data.shape)\ntest_data = tfidf_head_test + tfidf_des_test\nprint(test_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "122b06a3095bf2fda17d2bf02e0d85f150759bc1"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\n\nclf = LogisticRegression()\n#params we need to try on classifier\nparam_grid = { 'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n              'penalty':['l1','l2']}\nkf = KFold(n_splits=3) #For k fold splitting\ngsv = RandomizedSearchCV(clf,param_grid,cv= kf,verbose=1, n_jobs = -1)\ngsv.fit(train_data,df_train_label)\nsavetofile(gsv,\"Log Reg/gsv_tf_idf\")\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c6bba2af3c7fa337f16cc20da9d00df4b6fb592"
      },
      "cell_type": "code",
      "source": "#Testing Accuracy on Test data\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C= 1, penalty= 'l2')\nclf.fit(train_data,df_train_label)\ny_pred = clf.predict(test_data)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(df_test_label, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(df_test_label, y_pred, average = 'micro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(df_test_label, y_pred, average = 'micro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(df_test_label, y_pred,  average = 'micro')))\nprint(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\nprint(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\ndf_cm = pd.DataFrame(confusion_matrix(df_test_label, y_pred), range(4),range(4))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be13bb3ce95652836363e085067af3331617b730"
      },
      "cell_type": "code",
      "source": "gsv = openfromfile(\"Log Reg/gsv_tf_idf\")\nplot_error_vs_c(gsv)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04a6fc1f6fe889f8f312f153d94399da0cf5b95a"
      },
      "cell_type": "markdown",
      "source": "## Word2Vec"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94195355f28dfbea928c592ac38552ab2ac3ca1f"
      },
      "cell_type": "code",
      "source": "# Train your own Word2Vec model using your own text corpus\ni=0\nlist_of_sentence=[]\nfor sentence in preprocessed_description:\n    list_of_sentence.append(sentence.split())\n# min_count = 5 considers only words that occured atleast 5 times\nw2v_model=Word2Vec(list_of_sentence,min_count=5,size=50, workers=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c63011cccd60d985c6e10e753debc659a5c44286",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "\n\n\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8291f038a57be8f17b674ba51a1b24d68d6e086d"
      },
      "cell_type": "code",
      "source": "w2v_words = list(w2v_model.wv.vocab)\nprint(\"Number of words that occured minimum 5 times \",len(w2v_words))\nprint(\"Sample words \", w2v_words[0:50])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6487b537d66df2b23dfa36eb0282dbdf57c0d3fc"
      },
      "cell_type": "markdown",
      "source": "##  Converting text into vectors using wAvg W2V, TFIDF-W2V"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1c28a5cc76734bace052cf3c268ca876cf8e017"
      },
      "cell_type": "code",
      "source": "# average Word2Vec\n# compute average word2vec for each description.\nsent_vectors = []; # the avg-w2v for each description is stored in this list\nfor sent in tqdm(list_of_sentence): # for each description\n    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n    cnt_words =0; # num of words with a valid vector in the description\n    for word in sent: # for each word in a description\n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec /= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f3798d2911eaadf53401d18e559308d042858174"
      },
      "cell_type": "markdown",
      "source": "## TF-IDF weighted W2v"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5379843ccd739e66d13a0cfa79594c3da3ab8355"
      },
      "cell_type": "code",
      "source": "model = TfidfVectorizer()\nmodel.fit(preprocessed_description)\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(model.get_feature_names(), list(model.idf_)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "886aa9737658c627ecfb71127979c36beaefaa24"
      },
      "cell_type": "code",
      "source": "# TF-IDF weighted Word2Vec\ntfidf_feat = model.get_feature_names() # tfidf words/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\nrow=0;\nfor sent in tqdm(list_of_sentence): # for each review/sentence \n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence/review\n    for word in sent: # for each word in a review/sentence\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2v_model.wv[word]\n#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n            # to reduce the computation we are \n            # dictionary[word] = idf value of word in whole courpus\n            # sent.count(word) = tf valeus of word in this review\n            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec /= weight_sum\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b55fa0b6d656ed50d364bd3672d3e400154585c9"
      },
      "cell_type": "markdown",
      "source": "# ** Logistic Regression Model **"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e6612c5abd1e52fd4220f7b3c344856113ff6fa"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7caea627d9f5cd5a2e69d1a5023bad52f7577d79"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}